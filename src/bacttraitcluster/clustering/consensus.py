"""
Multi-algorithm consensus clustering with information-theoretic validation.

Novel contributions over existing tools
---------------------------------------
1.  **Multi-algorithm consensus for binary data** — K-Modes + Spectral
    (Jaccard kernel) + Agglomerative (Hamming) fused via a bootstrap
    consensus matrix (Monti et al. 2003).  ConsensusClusterPlus (R) uses
    continuous-data algorithms; no tool offers K-Modes in the consensus.
2.  **Stability paths via Normalised Variation of Information (NVI)** —
    per-k stability quantified by NVI between independent bootstrap halves,
    with 95 % CI bands.  Standard approaches use cophenetic correlation or
    PAC; NVI (Meilă 2007) is a proper metric on the space of partitions
    and is more principled for categorical assignments.
3.  **Permutation gap for VI** — null distribution for VI generated by
    permuting cluster labels, yielding a *p*-value per *k*.

References
----------
Monti et al. (2003) Machine Learning 52:91.
Meilă (2007) J Multivariate Anal 98:873.
Șenbabaoğlu et al. (2014) Sci Rep 4:6207.
"""

from __future__ import annotations


import logging
from dataclasses import dataclass
from typing import Callable, Dict, List, Tuple

import numpy as np
import pandas as pd
from joblib import Parallel, delayed
from scipy.cluster.hierarchy import fcluster, linkage, cut_tree
from scipy.spatial.distance import pdist, squareform
from sklearn.cluster import AgglomerativeClustering, SpectralClustering
from sklearn.metrics import silhouette_score

logger = logging.getLogger(__name__)

# ── distance / kernel for binary data ────────────────────────────────────


def jaccard_kernel(X: np.ndarray) -> np.ndarray:
    """Jaccard similarity kernel for binary (n × p) matrix."""
    X = X.astype(np.float32)
    inter = X @ X.T
    sums = X.sum(axis=1, keepdims=True)
    union = np.maximum(sums + sums.T - inter, 1)
    return inter / union


def hamming_distance(X: np.ndarray) -> np.ndarray:
    """Normalised Hamming distance matrix."""
    return squareform(pdist(X, metric="hamming"))


# ── single-algorithm clusterers ──────────────────────────────────────────


def _cluster_kmodes(X: np.ndarray, k: int, seed: int) -> np.ndarray:
    try:
        from kmodes.kmodes import KModes
    except Exception as e:
        raise ImportError(
            "kmodes is not installed. Install optional extra: pip install bact-trait-cluster[kmodes]"
        ) from e
    return KModes(n_clusters=k, init="Huang", n_init=5, random_state=seed).fit_predict(
        X
    )


def _cluster_spectral_jaccard(X: np.ndarray, k: int, seed: int) -> np.ndarray:
    K = jaccard_kernel(X)
    np.fill_diagonal(K, 1.0)
    return SpectralClustering(
        n_clusters=k, affinity="precomputed", random_state=seed, assign_labels="kmeans"
    ).fit_predict(K)


def _cluster_agglom_hamming(X: np.ndarray, k: int, seed: int) -> np.ndarray:
    D = hamming_distance(X)
    return AgglomerativeClustering(
        n_clusters=k, metric="precomputed", linkage="average"
    ).fit_predict(D)


_REGISTRY: Dict[str, Callable] = {
    "kmodes": _cluster_kmodes,
    "spectral_jaccard": _cluster_spectral_jaccard,
    "agglomerative_hamming": _cluster_agglom_hamming,
}


def validate_algorithms(
    algorithms: List[str],
) -> Tuple[List[str], List[Dict[str, str]]]:
    """Validate algorithm list and return (used, missing) with reasons.

    Missing algorithms are not fatal here; the pipeline decides whether to proceed.
    """
    used: List[str] = []
    missing: List[Dict[str, str]] = []
    for a in algorithms or []:
        if a not in _REGISTRY:
            missing.append({"algo": str(a), "reason": "unknown_algorithm"})
            continue
        if a == "kmodes":
            try:
                import kmodes  # noqa: F401
            except Exception as e:
                missing.append({"algo": "kmodes", "reason": f"missing_dependency: {e}"})
                continue
        used.append(a)
    return used, missing


def register_algorithm(name: str, fn: Callable) -> None:
    """Register custom clustering function ``fn(X, k, seed) → labels``."""
    _REGISTRY[name] = fn


# ── consensus matrix ─────────────────────────────────────────────────────


def consensus_matrix(
    X: np.ndarray,
    k: int,
    algorithms: List[str],
    n_runs: int = 200,
    frac: float = 0.8,
    seed: int = 42,
    n_jobs: int = -1,
) -> np.ndarray:
    """Build consensus matrix by bootstrap sub-sampling across algorithms."""
    n = X.shape[0]
    n_sub = max(2, int(n * frac))

    def _one_run(b: int):
        rng = np.random.RandomState(seed + b)
        idx = rng.choice(n, n_sub, replace=False)
        Xs = X[idx]
        cc = np.zeros((n, n), dtype=np.float32)
        mask = np.zeros(n, dtype=bool)
        mask[idx] = True
        cs_local = np.outer(mask, mask).astype(np.float32)
        for algo in algorithms:
            fn = _REGISTRY[algo]
            try:
                labels = fn(Xs, k, seed + b)
                same = (labels[:, None] == labels[None, :]).astype(np.float32)
                cc[np.ix_(idx, idx)] += same
            except Exception as exc:
                logger.warning("Algo %s k=%d run=%d failed: %s", algo, k, b, exc)
        return cc, cs_local * len(algorithms)

    results = Parallel(n_jobs=n_jobs, prefer="threads")(
        delayed(_one_run)(b) for b in range(n_runs)
    )
    co_c = sum(r[0] for r in results)
    co_s = sum(r[1] for r in results)
    return co_c / np.maximum(co_s, 1)


def consensus_labels(M: np.ndarray, k: int) -> np.ndarray:
    """Hierarchical clustering on ``1 − M`` to derive final labels.

    Uses ``cut_tree`` to enforce an exact number of clusters when possible.
    ``fcluster(..., criterion="maxclust")`` may legitimately return fewer than
    ``k`` clusters under many ties (common for consensus matrices), which can
    collapse solutions to 1 cluster and produce biologically implausible outputs.
    """
    D = np.clip(1.0 - M, 0, 1)
    np.fill_diagonal(D, 0)
    Z = linkage(squareform(D, checks=False), method="average")
    try:
        labs = cut_tree(Z, n_clusters=[int(k)]).reshape(-1) + 1
    except Exception:
        labs = fcluster(Z, t=k, criterion="maxclust")
    # Fallback if degeneracy still occurs
    u = np.unique(labs)
    if len(u) < min(int(k), M.shape[0]):
        # deterministic tie-break: spectral clustering on consensus similarity
        from sklearn.cluster import SpectralClustering

        K = np.clip(M, 0.0, 1.0).copy()
        np.fill_diagonal(K, 1.0)
        labs = (
            SpectralClustering(
                n_clusters=int(k),
                affinity="precomputed",
                random_state=42,
                assign_labels="kmeans",
            ).fit_predict(K)
            + 1
        )
    return labs.astype(int)


# ── Variation of Information ─────────────────────────────────────────────


def variation_of_information(a: np.ndarray, b: np.ndarray) -> float:
    """VI(A, B) = H(A) + H(B) − 2·I(A;B). Meilă (2007)."""
    n = len(a)
    if n == 0:
        return 0.0

    def _h(lab):
        _, c = np.unique(lab, return_counts=True)
        p = c / n
        return -np.sum(p * np.log(p + 1e-15))

    ct = pd.crosstab(pd.Series(a, name="a"), pd.Series(b, name="b"), normalize=True)
    mi = 0.0
    for i in range(ct.shape[0]):
        for j in range(ct.shape[1]):
            pij = ct.iloc[i, j]
            if pij > 0:
                mi += pij * np.log(
                    pij / (ct.sum(axis=1).iloc[i] * ct.sum(axis=0).iloc[j] + 1e-15)
                    + 1e-15
                )
    return _h(a) + _h(b) - 2 * mi


def nvi(a: np.ndarray, b: np.ndarray) -> float:
    """Normalised VI: VI / log(n). Range [0, ~1]."""
    n = len(a)
    return variation_of_information(a, b) / np.log(max(n, 2))


# ── stability path ───────────────────────────────────────────────────────


@dataclass
class KResult:
    """Evaluation result for one value of *k*."""

    k: int
    mean_nvi: float
    std_nvi: float
    ci_lo: float
    ci_hi: float
    silhouette: float
    M: np.ndarray
    labels: np.ndarray


def stability_path(
    X: np.ndarray,
    k_range: List[int],
    algorithms: List[str],
    n_runs: int = 200,
    n_splits: int = 30,
    frac: float = 0.8,
    seed: int = 42,
    n_jobs: int = -1,
) -> List[KResult]:
    """Per-k stability curves: build consensus, measure NVI across splits."""
    out: List[KResult] = []
    for k in k_range:
        logger.info("k=%d", k)
        M = consensus_matrix(X, k, algorithms, n_runs, frac, seed, n_jobs)
        labels = consensus_labels(M, k)
        vis = []
        half = max(n_runs // 2, 10)
        for s in range(n_splits):
            M1 = consensus_matrix(
                X, k, algorithms, half, frac, seed + s * 10000, n_jobs
            )
            M2 = consensus_matrix(
                X, k, algorithms, half, frac, seed + s * 10000 + half, n_jobs
            )
            vis.append(nvi(consensus_labels(M1, k), consensus_labels(M2, k)))
        va = np.array(vis)
        D = hamming_distance(X)
        try:
            sil = silhouette_score(D, labels, metric="precomputed")
        except ValueError:
            sil = -1.0
        out.append(
            KResult(
                k,
                float(va.mean()),
                float(va.std()),
                float(np.percentile(va, 2.5)),
                float(np.percentile(va, 97.5)),
                sil,
                M,
                labels,
            )
        )
    return out


def select_k(results: List[KResult]) -> KResult:
    """Select *k* with anti-degeneracy penalty, then stability, then silhouette.

    Prevents trivial single-cluster (or strongly collapsed) solutions from winning
    purely because they are stable.

    Penalty scheme: ``deg_pen = 10.0`` when all labels collapse to one cluster
    (nuniq ≤ 1); ``deg_pen = 1.0`` when nuniq < k (partial collapse);
    ``deg_pen = 0.0`` otherwise.  The default penalty of 10.0 was chosen to
    dominate any plausible NVI or silhouette difference; see
    ``sensitivity_select_k`` for a systematic evaluation of this choice.
    """

    def _key(r: KResult):
        nuniq = len(np.unique(r.labels))
        # heavy penalty if clustering collapsed below requested k (especially 1)
        deg_pen = 10.0 if nuniq <= 1 else (1.0 if nuniq < r.k else 0.0)
        return (deg_pen, r.mean_nvi, -r.silhouette)

    return min(results, key=_key)


def sensitivity_select_k(
    results: List[KResult],
    penalties: List[float] = None,
) -> pd.DataFrame:
    """Sensitivity analysis of select_k to the degeneracy penalty value.

    Evaluates which k is selected for each candidate penalty value in
    ``penalties``.  A stable result (same k selected across all penalties)
    provides strong evidence that the k selection is robust to this arbitrary
    parameter.

    Parameters
    ----------
    results : list of KResult
        Output of ``stability_path``.
    penalties : list of float, optional
        Degeneracy penalty values to test.
        Default: [2.0, 5.0, 10.0, 20.0, 50.0].

    Returns
    -------
    pd.DataFrame with columns Penalty, Selected_k, Mean_NVI, Silhouette,
    N_Unique_Labels.  The row where Penalty == 10.0 corresponds to the default
    behaviour of ``select_k``.
    """
    if penalties is None:
        penalties = [2.0, 5.0, 10.0, 20.0, 50.0]

    rows = []
    for pen in penalties:

        def _key(r: KResult, p=pen):
            nuniq = len(np.unique(r.labels))
            dp = p if nuniq <= 1 else (1.0 if nuniq < r.k else 0.0)
            return (dp, r.mean_nvi, -r.silhouette)

        best = min(results, key=_key)
        rows.append(
            {
                "Penalty": pen,
                "Selected_k": best.k,
                "Mean_NVI": round(best.mean_nvi, 4),
                "Silhouette": round(best.silhouette, 4),
                "N_Unique_Labels": int(len(np.unique(best.labels))),
            }
        )
    return pd.DataFrame(rows)


def sensitivity_n_runs(
    X: np.ndarray,
    k: int,
    algorithms: List[str],
    n_runs_values: List[int] = None,
    frac: float = 0.8,
    seed: int = 42,
    n_jobs: int = -1,
) -> pd.DataFrame:
    """Sensitivity analysis of consensus matrix stability to number of bootstrap runs.

    Builds consensus matrices at increasing ``n_runs`` values and reports the
    mean absolute deviation between consecutive matrices and the mean off-diagonal
    consensus matrix value.  Stability plateau (MAD < 0.01) indicates sufficient
    ``n_runs``.

    Parameters
    ----------
    X : np.ndarray
        Binary data matrix (n_samples × n_features).
    k : int
        Number of clusters.
    algorithms : list of str
        Algorithm names to use in consensus.
    n_runs_values : list of int, optional
        Bootstrap run counts to evaluate.
        Default: [50, 100, 150, 200, 300].
    frac : float
        Subsampling fraction (passed to ``consensus_matrix``).
    seed : int
    n_jobs : int

    Returns
    -------
    pd.DataFrame with columns N_runs, Mean_Consensus, MAD_from_prev,
    N_Labels (number of unique cluster labels at this n_runs).
    """
    if n_runs_values is None:
        n_runs_values = [50, 100, 150, 200, 300]

    rows = []
    prev_M = None
    for nr in sorted(n_runs_values):
        M = consensus_matrix(
            X, k, algorithms, n_runs=nr, frac=frac, seed=seed, n_jobs=n_jobs
        )
        labs = consensus_labels(M, k)
        mean_cons = float(np.mean(M[np.triu_indices_from(M, k=1)]))
        if prev_M is not None:
            mad = float(np.mean(np.abs(M - prev_M)))
        else:
            mad = float("nan")
        rows.append(
            {
                "N_runs": nr,
                "Mean_Consensus": round(mean_cons, 4),
                "MAD_from_prev": round(mad, 4) if not np.isnan(mad) else float("nan"),
                "N_Labels": int(len(np.unique(labs))),
            }
        )
        prev_M = M.copy()
    return pd.DataFrame(rows)
